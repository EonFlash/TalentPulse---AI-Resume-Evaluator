{"resume_data": "Name: Karan Verma\nEmail: karan.verma.test@example.com\nPhone: +91-98765-43210\nLocation: Bengaluru, India\nTotal Experience: 6 years\n\nProfessional Summary\nResult-driven Data Engineer with 6 years of experience building ETL pipelines, data warehouses and analytics platforms. Strong background in Python, SQL, Spark and cloud data services. Proven track record delivering production-ready data products that improved reporting speed and data quality.\n\nWork Experience\nSenior Data Engineer \u00e2\u20ac\u201d Nimbus Analytics, Bengaluru\nJan 2022 \u00e2\u20ac\u201d Present\n- Designed and implemented Spark-based ETL pipelines to process 500M+ records/week.\n- Migrated legacy data pipelines to cloud infrastructure (AWS Redshift + S3), reducing job runtime by 45%.\n- Built data quality monitoring and alerting using Airflow and Great Expectations.\n\nData Engineer \u00e2\u20ac\u201d BlueByte Solutions, Pune\nJul 2019 \u00e2\u20ac\u201d Dec 2021\n- Developed scalable ingestion pipelines using Python, Kafka and Spark.\n- Created dimensional models and optimized queries for BI reporting, improving dashboard latency by 30%.\n- Collaborated with ML team to operationalize feature stores.\n\nAssociate Data Engineer \u00e2\u20ac\u201d TechNova, Hyderabad\nJun 2017 \u00e2\u20ac\u201d Jun 2019\n- Built automated data validation and ingestion processes.\n- Wrote reusable SQL transformations and scheduled jobs with Apache Airflow.\n\nEducation\nB.Tech, Computer Science \u00e2\u20ac\u201d Visvesvaraya Technological University\n2013 \u00e2\u20ac\u201d 2017\n\nSkills\n- Languages: Python, SQL\n- Big Data: Apache Spark, Kafka\n- Orchestration: Apache Airflow\n- Cloud: AWS (S3, Redshift, EMR)\n- Tools: dbt, Great Expectations, Git\n\nCertifications\n- AWS Certified Data Analytics \u00e2\u20ac\u201c Specialty (mock)\n- Databricks Certified Associate Developer for Apache Spark (mock)\n", "job_description": "Job Title: Senior Data Engineer\nLocation: Bengaluru, India\nExperience: 5\u00e2\u20ac\u201c7 years\n\nResponsibilities:\n\nDesign, develop, and maintain ETL pipelines and data workflows.\n\nWork with large-scale data in AWS Redshift, S3, and Spark.\n\nOptimize data storage and retrieval for BI reporting and analytics.\n\nCollaborate with ML and analytics teams to build feature stores and dashboards.\n\nImplement data quality monitoring using Airflow and Great Expectations.\n\nRequirements:\n\nStrong proficiency in Python and SQL.\n\nExperience with Apache Spark, Kafka, and Airflow.\n\nKnowledge of cloud platforms, preferably AWS.\n\nFamiliarity with dbt, Git, and version control workflows.\n\nExcellent problem-solving and communication skills.", "match_percentage": 95, "candidate_name": "Karan Verma", "experience": 6}